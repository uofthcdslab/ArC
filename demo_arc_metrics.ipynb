{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArC (Argument-based Consistency) Metrics Demo\n",
    "\n",
    "This notebook demonstrates how to use the ArC framework to compute and analyze toxicity explanation metrics for LLMs.\n",
    "\n",
    "## Overview\n",
    "\n",
    "ArC evaluates LLM reasoning about toxicity through six metrics across four dimensions:\n",
    "\n",
    "1. **Relevance Dimension**: SoS (Sufficiency of Stance), DiS (Diversity of Stance)\n",
    "2. **Internal Reliance**: UII (Uncertainty in Internal Informativeness)\n",
    "3. **External Reliance**: UEI (Uncertainty in External Informativeness)\n",
    "4. **Individual Reliance**: RS (Reason Sufficiency), RN (Reason Necessity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import ArC components\n",
    "from core.models.arc_config import ArCConfig\n",
    "from services.arc_service import ArCService\n",
    "from utils import helpers as hp\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Setup\n",
    "\n",
    "First, let's create an ArC configuration with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ArC configuration with default parameters\n",
    "config = ArCConfig.from_args(\n",
    "    explicit_prompting=True,  # Use explicit reasoning instructions\n",
    "    use_scores=False,         # Use logits-based entropy (not scores)\n",
    "    similarity_model='cross-encoder/stsb-distilroberta-base'\n",
    ")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Explicit Prompting: {config.explicit_prompting}\")\n",
    "print(f\"  Entropy Mode: {config.entropy_mode}\")\n",
    "print(f\"  Similarity Model: {config.similarity_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize ArC Service\n",
    "\n",
    "The ArCService handles all metric computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ArC service\n",
    "service = ArCService(config)\n",
    "\n",
    "print(f\"Available models: {service.model_names}\")\n",
    "print(f\"Available datasets: {service.data_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Pre-computed Results\n",
    "\n",
    "Let's load and examine pre-computed ArC results for a specific model and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model and dataset\n",
    "model_name = \"Llama-3.1-8B-Instruct\"\n",
    "data_name = \"civil_comments\"\n",
    "\n",
    "# Load results\n",
    "results_path = Path(\"arc_results\") / model_name / data_name\n",
    "\n",
    "if results_path.exists():\n",
    "    # Load all result files\n",
    "    results = []\n",
    "    for pkl_file in sorted(results_path.glob(\"*.pkl\")):\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            sample_result = pickle.load(f)\n",
    "            sample_result['sample_idx'] = int(pkl_file.stem)\n",
    "            results.append(sample_result)\n",
    "    \n",
    "    print(f\"Loaded {len(results)} samples for {model_name} on {data_name}\")\n",
    "else:\n",
    "    print(f\"Results not found at {results_path}\")\n",
    "    print(\"Please run: python arc.py\")\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Examine a Single Sample\n",
    "\n",
    "Let's look at the detailed metrics for one sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Get first sample\n",
    "    sample = results[0]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Sample {sample['sample_idx']} Metrics\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Initial reasons\n",
    "    print(f\"\\nInitial Reasons ({len(sample.get('initial_reasons', []))})\")\n",
    "    for i, reason in enumerate(sample.get('initial_reasons', [])):\n",
    "        print(f\"  {i+1}. {reason[:100]}...\")\n",
    "    \n",
    "    # Decision confidences\n",
    "    print(f\"\\nDecision Confidences:\")\n",
    "    print(f\"  Initial: {sample.get('initial_decision_confidence', 'N/A'):.4f}\")\n",
    "    print(f\"  Internal: {sample.get('internal_decision_confidence', 'N/A'):.4f}\")\n",
    "    print(f\"  External: {sample.get('external_decision_confidence', 'N/A'):.4f}\")\n",
    "    \n",
    "    # SoS scores\n",
    "    print(f\"\\nSoS (Sufficiency of Stance):\")\n",
    "    if 'SoS' in sample:\n",
    "        for key, value in sample['SoS'].items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "    \n",
    "    # DiS scores\n",
    "    print(f\"\\nDiS (Diversity of Stance):\")\n",
    "    print(f\"  DiS-DPP: {sample.get('DiS_dpp', 'N/A')}\")\n",
    "    print(f\"  DiS-Avg: {sample.get('DiS_avg', 'N/A')}\")\n",
    "    \n",
    "    # UII scores\n",
    "    print(f\"\\nUII (Uncertainty in Internal Informativeness):\")\n",
    "    if 'UII' in sample:\n",
    "        for key, value in sample['UII'].items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "    print(f\"  Internal Δ-PE: {sample.get('internal_del_pe', 'N/A')}\")\n",
    "    \n",
    "    # UEI scores\n",
    "    print(f\"\\nUEI (Uncertainty in External Informativeness):\")\n",
    "    if 'UEI' in sample:\n",
    "        for key, value in sample['UEI'].items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "    print(f\"  External Δ-PE: {sample.get('external_del_pe', 'N/A')}\")\n",
    "    \n",
    "    # RS/RN scores\n",
    "    if 'RS' in sample:\n",
    "        print(f\"\\nRS (Reason Sufficiency):\")\n",
    "        for key, value in sample['RS'].items():\n",
    "            print(f\"  Subsample {key}: {value:.4f}\")\n",
    "    \n",
    "    if 'RN' in sample:\n",
    "        print(f\"\\nRN (Reason Necessity):\")\n",
    "        for key, value in sample['RN'].items():\n",
    "            print(f\"  Subsample {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aggregate Statistics Across All Samples\n",
    "\n",
    "Now let's compute summary statistics across all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Collect metrics\n",
    "    sos_values = []\n",
    "    dis_dpp_values = []\n",
    "    dis_avg_values = []\n",
    "    uii_values = []\n",
    "    uei_values = []\n",
    "    rs_values = []\n",
    "    rn_values = []\n",
    "    \n",
    "    initial_conf = []\n",
    "    internal_conf = []\n",
    "    external_conf = []\n",
    "    \n",
    "    for result in results:\n",
    "        # SoS\n",
    "        if 'SoS' in result:\n",
    "            sos_values.extend([v for v in result['SoS'].values() if not pd.isna(v)])\n",
    "        \n",
    "        # DiS\n",
    "        if 'DiS_dpp' in result and not pd.isna(result['DiS_dpp']):\n",
    "            dis_dpp_values.append(result['DiS_dpp'])\n",
    "        if 'DiS_avg' in result and not pd.isna(result['DiS_avg']):\n",
    "            dis_avg_values.append(result['DiS_avg'])\n",
    "        \n",
    "        # UII\n",
    "        if 'UII' in result:\n",
    "            uii_values.extend([v for v in result['UII'].values() if not pd.isna(v)])\n",
    "        \n",
    "        # UEI\n",
    "        if 'UEI' in result:\n",
    "            uei_values.extend([v for v in result['UEI'].values() if not pd.isna(v)])\n",
    "        \n",
    "        # RS\n",
    "        if 'RS' in result:\n",
    "            rs_values.extend([v for v in result['RS'].values() if not pd.isna(v)])\n",
    "        \n",
    "        # RN\n",
    "        if 'RN' in result:\n",
    "            rn_values.extend([v for v in result['RN'].values() if not pd.isna(v)])\n",
    "        \n",
    "        # Confidences\n",
    "        if 'initial_decision_confidence' in result:\n",
    "            initial_conf.append(result['initial_decision_confidence'])\n",
    "        if 'internal_decision_confidence' in result:\n",
    "            internal_conf.append(result['internal_decision_confidence'])\n",
    "        if 'external_decision_confidence' in result:\n",
    "            external_conf.append(result['external_decision_confidence'])\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = {\n",
    "        'Metric': ['SoS', 'DiS-DPP', 'DiS-Avg', 'UII', 'UEI', 'RS', 'RN'],\n",
    "        'Mean': [\n",
    "            np.mean(sos_values) if sos_values else np.nan,\n",
    "            np.mean(dis_dpp_values) if dis_dpp_values else np.nan,\n",
    "            np.mean(dis_avg_values) if dis_avg_values else np.nan,\n",
    "            np.mean(uii_values) if uii_values else np.nan,\n",
    "            np.mean(uei_values) if uei_values else np.nan,\n",
    "            np.mean(rs_values) if rs_values else np.nan,\n",
    "            np.mean(rn_values) if rn_values else np.nan,\n",
    "        ],\n",
    "        'Std': [\n",
    "            np.std(sos_values) if sos_values else np.nan,\n",
    "            np.std(dis_dpp_values) if dis_dpp_values else np.nan,\n",
    "            np.std(dis_avg_values) if dis_avg_values else np.nan,\n",
    "            np.std(uii_values) if uii_values else np.nan,\n",
    "            np.std(uei_values) if uei_values else np.nan,\n",
    "            np.std(rs_values) if rs_values else np.nan,\n",
    "            np.std(rn_values) if rn_values else np.nan,\n",
    "        ],\n",
    "        'Min': [\n",
    "            np.min(sos_values) if sos_values else np.nan,\n",
    "            np.min(dis_dpp_values) if dis_dpp_values else np.nan,\n",
    "            np.min(dis_avg_values) if dis_avg_values else np.nan,\n",
    "            np.min(uii_values) if uii_values else np.nan,\n",
    "            np.min(uei_values) if uei_values else np.nan,\n",
    "            np.min(rs_values) if rs_values else np.nan,\n",
    "            np.min(rn_values) if rn_values else np.nan,\n",
    "        ],\n",
    "        'Max': [\n",
    "            np.max(sos_values) if sos_values else np.nan,\n",
    "            np.max(dis_dpp_values) if dis_dpp_values else np.nan,\n",
    "            np.max(dis_avg_values) if dis_avg_values else np.nan,\n",
    "            np.max(uii_values) if uii_values else np.nan,\n",
    "            np.max(uei_values) if uei_values else np.nan,\n",
    "            np.max(rs_values) if rs_values else np.nan,\n",
    "            np.max(rn_values) if rn_values else np.nan,\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(\"\\nArC Metrics Summary:\")\n",
    "    print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Metric Distributions\n",
    "\n",
    "Let's create visualizations of the metric distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle(f'ArC Metrics Distribution - {model_name} on {data_name}', fontsize=16)\n",
    "    \n",
    "    # Plot distributions\n",
    "    metrics_to_plot = [\n",
    "        (sos_values, 'SoS', axes[0, 0]),\n",
    "        (uii_values, 'UII', axes[0, 1]),\n",
    "        (uei_values, 'UEI', axes[0, 2]),\n",
    "        (dis_avg_values, 'DiS-Avg', axes[1, 0]),\n",
    "        (rs_values, 'RS', axes[1, 1]),\n",
    "        (rn_values, 'RN', axes[1, 2]),\n",
    "    ]\n",
    "    \n",
    "    for values, name, ax in metrics_to_plot:\n",
    "        if values:\n",
    "            ax.hist(values, bins=30, edgecolor='black', alpha=0.7)\n",
    "            ax.set_xlabel(name)\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.set_title(f'{name} Distribution')\n",
    "            ax.axvline(np.mean(values), color='red', linestyle='--', label=f'Mean: {np.mean(values):.3f}')\n",
    "            ax.legend()\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title(f'{name} Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Decision Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results and initial_conf:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    confidence_data = {\n",
    "        'Initial': initial_conf,\n",
    "        'Internal': internal_conf,\n",
    "        'External': external_conf\n",
    "    }\n",
    "    \n",
    "    positions = [1, 2, 3]\n",
    "    bp = ax.boxplot([confidence_data['Initial'], confidence_data['Internal'], confidence_data['External']], \n",
    "                     positions=positions, labels=['Initial', 'Internal', 'External'])\n",
    "    \n",
    "    ax.set_ylabel('Confidence')\n",
    "    ax.set_title('Decision Confidence Across Stages')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nDecision Confidence Statistics:\")\n",
    "    for stage, values in confidence_data.items():\n",
    "        if values:\n",
    "            print(f\"  {stage}: Mean={np.mean(values):.4f}, Std={np.std(values):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Computing Metrics for New Data\n",
    "\n",
    "Here's how to compute ArC metrics for a single sample programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compute metrics for a specific sample\n",
    "sample_idx = 0\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "data_name = \"civil_comments\"\n",
    "\n",
    "# Load output tokens and parsed outputs\n",
    "output_tokens_dict = hp.get_output_tokens(model_name, data_name, config.explicit_prompting)\n",
    "parsed_output_dict = hp.get_parsed_outputs(model_name, data_name, config.explicit_prompting)\n",
    "\n",
    "# Compute metrics for the sample\n",
    "sample_result = service.compute_sample(\n",
    "    sample_idx, model_name, data_name,\n",
    "    output_tokens_dict, parsed_output_dict\n",
    ")\n",
    "\n",
    "print(f\"\\nComputed metrics for sample {sample_idx}:\")\n",
    "print(f\"  Number of initial reasons: {len(sample_result.get('initial_reasons', []))}\")\n",
    "print(f\"  Initial decision confidence: {sample_result.get('initial_decision_confidence', 'N/A')}\")\n",
    "print(f\"  SoS scores: {sample_result.get('SoS', {})}\")\n",
    "print(f\"  DiS-DPP: {sample_result.get('DiS_dpp', 'N/A')}\")\n",
    "print(f\"  DiS-Avg: {sample_result.get('DiS_avg', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparing Multiple Models\n",
    "\n",
    "Load and compare results from different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple models\n",
    "models_to_compare = [\"Llama-3.1-8B-Instruct\", \"Llama-3.2-3B-Instruct\", \"Ministral-8B-Instruct-2410\"]\n",
    "data_name = \"civil_comments\"\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for model in models_to_compare:\n",
    "    results_path = Path(\"arc_results\") / model / data_name\n",
    "    if results_path.exists():\n",
    "        results = []\n",
    "        for pkl_file in sorted(results_path.glob(\"*.pkl\")):\n",
    "            with open(pkl_file, 'rb') as f:\n",
    "                results.append(pickle.load(f))\n",
    "        \n",
    "        # Collect SoS values\n",
    "        sos_vals = []\n",
    "        for r in results:\n",
    "            if 'SoS' in r:\n",
    "                sos_vals.extend([v for v in r['SoS'].values() if not pd.isna(v)])\n",
    "        \n",
    "        if sos_vals:\n",
    "            comparison_data.append({\n",
    "                'Model': model,\n",
    "                'SoS_Mean': np.mean(sos_vals),\n",
    "                'SoS_Std': np.std(sos_vals),\n",
    "                'Samples': len(results)\n",
    "            })\n",
    "\n",
    "if comparison_data:\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\nModel Comparison (SoS Metric):\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.bar(comparison_df['Model'], comparison_df['SoS_Mean'], yerr=comparison_df['SoS_Std'], capsize=5)\n",
    "    ax.set_ylabel('SoS Mean')\n",
    "    ax.set_title('SoS Comparison Across Models')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. How to configure and initialize the ArC framework\n",
    "2. Loading and examining pre-computed results\n",
    "3. Analyzing individual sample metrics\n",
    "4. Computing aggregate statistics across samples\n",
    "5. Visualizing metric distributions\n",
    "6. Programmatically computing metrics for new samples\n",
    "7. Comparing results across different models\n",
    "\n",
    "For more information, see the [ArC paper](https://arxiv.org/pdf/2506.19113) and the project README."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
